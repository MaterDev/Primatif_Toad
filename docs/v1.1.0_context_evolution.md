# v1.1.0: Context Evolution & AI Synergy

## Vision

The v1.1.0 update focuses on evolving the data generated by Toad into a high-fidelity, live-syncing context layer. This layer is specifically designed to be consumed by AI agents (like Gemini, Claude, and local LLMs) to perform meta-operations with extreme accuracy and semantic awareness.

This aligns with the industry shift from naive RAG to **Context Engineering** — "the art and science of filling the context window with just the right information at each step of an agent's trajectory." Toad's role is to be the **semantic layer** for local development ecosystems: attaching structured, machine-readable metadata to all project data so that agents can consistently understand, retrieve, and reason over it.

## Core Objectives

1. **High-Density Context:** Improve the depth and relevance of data stored in `~/.toad/shadows/`.
2. **AI-Native Structures:** Format context data (MANIFEST.md, JSON schemas) specifically for agentic consumption.
3. **Maintenance & Freshness:** Implement robust mechanisms to ensure context remains accurate as repositories evolve.
4. **Reference-Driven Ops:** Enable Toad to use this context internally to judge the impact and feasibility of batch jobs, macros, and transformations.
5. **Agent Interoperability:** Generate and expose context in formats compatible with the broader AI tooling ecosystem (AGENTS.md, MCP, RAG-ready chunks).

## Non-Goals

* Toad is **not** a code intelligence server (like Sourcegraph). It does not perform AST parsing or cross-language symbol resolution.
* Toad is **not** a vector database. It generates structured metadata that can be *piped into* vector stores, but does not embed or index vectors itself.
* Toad is **not** an AI agent. It is the **context oracle** that agents query to become smarter about the ecosystem.

## Roadmap to v1.1.0

### Phase 0: Global Install Architecture

*Goal: Decouple Toad's generated context from the source repo so the CLI works as a standalone global install. The user should never need the Primatif_Toad repository on their machine to use Toad.*

*Depends on: Nothing (prerequisite for all other phases).*

*Success Metrics: `toad` runs from any directory after `cargo install`. All generated context lives in `~/.toad/`. The user's project directory is never polluted with Toad metadata.*

#### The Problem

Currently, `Workspace` assumes a layout where `shadows/` and `projects/` are siblings under a single root (the Primatif_Toad repo). This means:

* `shadows/` lives inside the workspace root, polluting the user's directory.
* The user must have the Primatif_Toad repo cloned to use the tool.
* Context files are tied to a specific repo checkout, not portable.

#### The New Architecture

Separate **Toad's home** (where it stores its own data) from the **user's project directory** (where their repos live):

```text
~/.toad/                              # Toad Home (global, managed by Toad)
├── config.json                       # Global config (home_pointer, settings)
├── registry.json                     # ProjectRegistry (fingerprint, project list)
├── shadows/                          # ALL generated context lives here
│   ├── llms.txt                      # Ecosystem table of contents
│   ├── SYSTEM_PROMPT.md              # Ecosystem-level AI briefing
│   ├── MANIFEST.md                   # Human-readable project table
│   ├── context.json                  # Machine-readable full context
│   ├── CHANGELOG.json                # Drift tracking
│   ├── tags.json                     # Tag registry
│   ├── ATLAS.json                    # Structural DNA & patterns (Phase 4)
│   ├── project-a/
│   │   ├── CONTEXT.md                # Project-level AI briefing
│   │   └── AGENTS.md                 # Agent instructions
│   ├── project-b/
│   │   ├── CONTEXT.md
│   │   └── AGENTS.md
│   └── ...
└── cache/                            # Optional: semantic cache, synthesis artifacts

~/Code/                               # User's Project Directory (untouched by Toad)
├── project-a/                        # Independent git repo
├── project-b/                        # Independent git repo
└── ...
```

#### Setup Flow

```bash
# 1. Install globally (no repo clone needed)
cargo install toad

# 2. Point Toad at the user's project directory
toad home ~/Code

# 3. Bootstrap context (one time, then self-managing)
toad init-context
```

#### Refactoring Required

1. **`Workspace` struct** — Split into two path concepts:

    * `toad_home: PathBuf` — always `~/.toad/` (or `$TOAD_HOME` override). This is where Toad stores everything it generates.
    * `projects_dir: PathBuf` — the user's directory containing repos. Stored in `config.json` as `home_pointer`. This is read-only from Toad's perspective (Toad never writes here except via `toad do` or `toad create`).

2. **`shadows_dir`** — Move from `{workspace_root}/shadows/` to `~/.toad/shadows/`. All references to `workspace.shadows_dir` must resolve to the Toad home, not the project directory.

3. **`tags_path`** — Move from `{workspace_root}/shadows/tags.json` to `~/.toad/shadows/tags.json`.

4. **`manifest_path`** — Move from `{workspace_root}/shadows/MANIFEST.md` to `~/.toad/shadows/MANIFEST.md`.

5. **`Workspace::discover()`** — Simplify the 3-tier priority:

    * Tier 1: `$TOAD_HOME` env var (overrides `~/.toad/`).
    * Tier 2: `~/.toad/config.json` → `home_pointer` (the user's project directory).
    * Tier 3: Error with clear setup instructions.
    * Remove the upward `.toad-root` file search — this is a repo-coupled pattern that doesn't apply to global installs.

6. **`GlobalConfig`** — Extend to store additional settings:

    ```json
    {
      "home_pointer": "/Users/jake/Code",
      "auto_sync": true,
      "context_budget": {
        "ecosystem_tokens": 2000,
        "project_tokens": 4000
      }
    }
    ```

7. **Tests** — Update `test_workspace_paths` and `test_workspace_discovery_tiers` to reflect the new path separation. Tests should use `tempdir` for both `toad_home` and `projects_dir` independently.

#### Backward Compatibility

* If `~/.toad/config.json` doesn't exist but a `.toad-root` file is found (old-style workspace), Toad auto-migrates: copies `shadows/` contents to `~/.toad/shadows/`, writes a new `config.json`, and prints a one-time migration notice.
* The `TOAD_ROOT` env var continues to work but now only sets the `projects_dir`, not the entire workspace root.

### Phase 1: Deep Extraction & Structured Data

*Goal: Provide a more detailed and machine-readable context.*

*Depends on: Phase 0 (global install architecture must be in place so context writes to `~/.toad/shadows/`).*

*Success Metrics: JSON context covers 100% of `ProjectDetail` fields. `~/.toad/shadows/context.json` is generated alongside `MANIFEST.md` on every `toad manifest` run.*

1. **Semantic Essence Extraction:**

    * Modify `extract_essence` to include headers (for context markers) and specifically look for "Capability" indicators (e.g., "Provides", "Exposes", "Main Entry").

    * Limit extraction by semantic value rather than just line count.

2. **Structured JSON Context:**

    * Implement `toad-manifest --json` (and make it the default side-effect of `toad manifest`).

    * Generate `shadows/context.json` containing the full `ProjectDetail` list plus new stats.

3. **Stats Integration:**

    * Include `bloat_index` and `total_size` in the `ProjectDetail` (and thus the manifest).

### Phase 2: Context Integrity & Maintenance

*Goal: Ensure the context is always truthful.*

*Depends on: Phase 1 (requires `context.json` to exist).*

*Success Metrics: Fingerprint comparison uses `!=` check. `toad manifest --check` exits non-zero on stale context. `shadows/CHANGELOG.json` tracks at least vcs_status and activity changes.*

1. **Reliable Staleness Detection:**

    * Fix the fingerprint comparison logic (change `>` to `!=` or a more robust hash check).

    * Add a `--check` flag to `toad manifest` to only report staleness without updating.

2. **Auto-Sync Triggers (Self-Management):**

    The user should never have to remember to regenerate context. Toad should manage this itself.

    * **Opportunistic Refresh:** Every Toad command (`status`, `reveal`, `do`, `stats`) checks the fingerprint on startup. If stale, Toad silently regenerates `shadows/` in the background before executing the command. The user never runs a separate sync step.

    * **Post-Mutation Hook:** After any `toad do` operation that modifies files, Toad automatically triggers a context refresh and prints a one-line summary: `Context updated (3 projects changed).`

    * **Watch Mode (Optional):** `toad sync --watch` as a lightweight daemon for users who want real-time sync. This is a nice-to-have, not the primary mechanism.

    * **Staleness Warning:** If context is stale and auto-refresh is disabled (e.g., `--no-sync` flag), Toad prints a visible warning: `⚠️ Context is stale. Run 'toad init-context' or remove --no-sync.`

3. **Diff-Aware Context (Changelog):**

    * Generate `shadows/CHANGELOG.json` that tracks what changed between syncs, not just whether something changed.

    * Track changes to: `vcs_status`, `activity` tier, new/removed projects, and new high-value files.

    * This lets AI agents understand **drift** without re-scanning everything.

### Phase 3: Agent Interface & Meta-Ops

*Goal: Bridge the gap between context and action. Make Toad's knowledge consumable by any AI tool.*

*Depends on: Phase 1 (JSON context), Phase 2 (trustworthy data).*

*Success Metrics: `AGENTS.md` generated for all detected stacks. Tiered prompt files exist. `toad do` warns on stack mismatches.*

1. **AGENTS.md Generation:**

    * Generate per-project `AGENTS.md` files from Toad's knowledge of each project's stack, build commands, test patterns, and taxonomy.

    * The `AGENTS.md` format is adopted by 60k+ open-source projects and supported by Codex, Gemini CLI, Cursor, Windsurf, Amp, Copilot, and more.

    * Commands: `toad agents` (all projects), `toad agents <name>` (single project).

2. **Tiered Prompt Generation (Progressive Disclosure):**

    * Generate context at multiple granularities instead of a single monolithic system prompt. The key principle is **progressive disclosure** — give the AI the minimum viable context first, and let it drill deeper only when needed. This keeps token usage low while maintaining accuracy.

    | Tier | File | Purpose | Token Budget |
    | :--- | :--- | :--- | :--- |
    | Entry | `shadows/llms.txt` | Table of contents — read this first | ~500 tokens |
    | Ecosystem | `shadows/SYSTEM_PROMPT.md` | Bird's-eye view of all projects | ~2k tokens |
    | Project | `shadows/{project}/CONTEXT.md` | Deep dive on one project | ~4k tokens |
    | Task | Generated on-demand | Relevant context for a specific operation | Dynamic |

    * **How an AI agent navigates this:**

        1. Agent reads `llms.txt` (~500 tokens). Now it knows what projects exist and where to look.
        2. Agent reads `SYSTEM_PROMPT.md` (~2k tokens) only if it needs ecosystem-wide understanding.
        3. Agent reads a specific `{project}/CONTEXT.md` (~4k tokens) only when working on that project.
        4. Agent queries MCP (Phase 3.5) for targeted data only when it needs a specific answer.

    * **Total cost for a focused task:** ~500 + ~4k = **~4.5k tokens** instead of dumping the entire ecosystem into context.
    * **Total cost for ecosystem-wide task:** ~500 + ~2k = **~2.5k tokens** for the overview, then drill into specific projects as needed.

3. **Context-Aware Pre-flights:**

    * Teach `toad do` to use the `context.json` to warn about potentially mismatched operations (e.g., running `cargo` on a project without a `Cargo.toml`).

4. **Semantic Reveal:**

    * Enhance `toad reveal` to search within the `essence` or tags more effectively using the cached registry.

5. **Context Initialization (`toad init-context`):**

    * A single command that bootstraps the entire AI onboarding pipeline for the ecosystem. Runs the full generation chain in order:

        1. `toad manifest` — scan projects, generate `context.json` + `MANIFEST.md`
        2. `toad agents` — generate per-project `AGENTS.md` files
        3. Tiered prompts — generate `SYSTEM_PROMPT.md` + per-project `CONTEXT.md`
        4. `llms.txt` — generate the ecosystem table of contents

    * **The result:** A fully populated `shadows/` directory that any AI agent can immediately consume:

        ```text
        shadows/
        ├── llms.txt                 # START HERE — ecosystem table of contents (~500 tokens)
        ├── SYSTEM_PROMPT.md         # Ecosystem-level AI briefing (~2k tokens)
        ├── MANIFEST.md              # Human-readable project table
        ├── context.json             # Machine-readable full context (for MCP/programmatic use)
        ├── CHANGELOG.json           # Drift tracking between syncs
        ├── project-a/
        │   ├── CONTEXT.md           # Deep project-level AI briefing (~4k tokens)
        │   └── AGENTS.md            # Agent instructions for this project
        ├── project-b/
        │   ├── CONTEXT.md
        │   └── AGENTS.md
        └── ...
        ```

    * **Onboarding an AI agent becomes a single instruction:** "Read `shadows/llms.txt` first, then follow its links for deeper context."

    * **Self-Managing:** After the initial `toad init-context`, the user never needs to run it again manually. The opportunistic refresh (Phase 2) keeps everything in sync automatically. Every Toad command checks the fingerprint and silently regenerates stale context.

    * **The only thing the user remembers:** Run `toad init-context` once after setup. After that, Toad handles it.

    * Supports `--dry-run` to preview what would be generated without writing files.
    * Supports `--project <name>` to regenerate context for a single project.
    * Supports `--no-sync` to disable auto-refresh for CI/scripting environments.

### Phase 3.5: MCP Server Mode (The Big Play)

*Goal: Transform Toad from a passive file generator into a live context oracle that AI agents can query directly.*

*Depends on: Phase 1 (structured data to serve), Phase 3 (agent interface patterns).*

*Success Metrics: Toad responds to MCP tool calls. At least 3 tools exposed: `list_projects`, `get_project_detail`, `search_projects`.*

1. **MCP Server Implementation:**

    * Expose Toad's knowledge as a Model Context Protocol (MCP) server that any compatible AI agent can query in real-time.

    * Example interactions:

        * Agent: "Which projects use Rust and have dirty git status?" → Toad returns filtered `ProjectDetail` list.
        * Agent: "What's the entry point for the auth logic?" → Toad queries Atlas, returns file paths.

    * The Rust ecosystem has MCP server libraries available. This is a natural extension of Toad's existing data models.

2. **Exposed Tools:**

    * `list_projects` — filtered by stack, activity, vcs_status, or tag.
    * `get_project_detail` — full context for a single project.
    * `search_projects` — semantic search across essence, tags, and taxonomy.
    * `get_ecosystem_summary` — the system prompt tier, on demand.

### Phase 4: Pattern Intelligence & Portability

*Goal: Facilitate migration, duplication, and synthesis across independent repositories.*

*Depends on: Phase 1 (structured data), Phase 3 (AGENTS.md patterns inform DNA mapping), Phase 3.5 (MCP serves as query interface for pattern data).*

*Success Metrics: `ATLAS.json` indexes "Structural DNA" and "Capability Patterns" across all repos. Context injection command available. `toad context --task` generates a valid Situation Report.*

1. **Structural DNA Mapping:**

    * Identify "Project Skeleton" roles (e.g., "Business logic lives in `src/logic`", "Models in `src/models`").
    * Help AI understand the "organs" of a project to make moving solutions from Repo A to Repo B seamless.

    * **Implementation Approach (No AST — Heuristic-Based):**

        Extend the existing `StackStrategy` pattern in `toad-core` to detect **structural roles** via file/directory naming conventions and grep-level text search:

        | Signal | What It Reveals | Detection Method |
        | :--- | :--- | :--- |
        | `src/models/`, `src/entities/` | Data layer location | Directory name matching |
        | `src/services/`, `src/logic/` | Business logic location | Directory name matching |
        | `src/routes/`, `src/handlers/` | API surface | Directory name matching |
        | `tests/`, `__tests__/`, `spec/` | Test organization | Directory name matching |
        | `Dockerfile`, `docker-compose.yml` | Containerized deployment | File existence |
        | `pub fn`, `export default`, `func` | Entry point declarations | Grep-level text search |

        This stays within the Non-Goals (no AST parsing) while providing high-value structural insight.

2. **Capability & Solution Indexing:**

    * Index "Recipes" and "Solvability Patterns" (e.g., "Uses JWT Auth", "Implements Actor Pattern", "Event-Driven").
    * Allow AI to find "how I solved this before" even if the projects are entirely unrelated.
    * This maps to what Google's internal migration team calls **"opportunity discovery"** — finding all the places where a pattern exists before transforming it.

3. **Architectural Clustering:**

    * Group projects by "Vibe" or "Pattern" (e.g., "High-Performance CLI", "CRUD Service", "Plugin-based Architecture").
    * Enable "Inspiration" searches: "Show me all projects that use this specific error-handling pattern."

4. **Context Injection & Synthesis:**

    * Implement a command `toad context --task "Migrate auth from A to B"` or `toad context --inspire "error handling"`.
    * **The Output:** Generates a single, condensed Markdown block — a "Situation Report" — ready to be pasted into an AI chat. Example format:

        ```markdown
        # Situation Report: Migrate auth from project-a to project-b

        ## Source: project-a
        - **Stack:** Rust (Actix-Web)
        - **DNA:** Business logic in `src/services/`, models in `src/models/`
        - **Auth Pattern:** JWT via `jsonwebtoken` crate, middleware in `src/middleware/auth.rs`
        - **Key Files:** `src/services/auth.rs`, `src/models/user.rs`

        ## Target: project-b
        - **Stack:** Rust (Axum)
        - **DNA:** Business logic in `src/handlers/`, models in `src/domain/`
        - **Compatibility Notes:**
          - Source uses `anyhow`, Target uses `thiserror` — error types need adaptation
          - Source uses Actix middleware, Target uses Axum extractors — pattern differs

        ## Relevant Snippets
        [extracted code from source entry points]
        ```

    * **Migration Pre-flights:** A utility to compare two repos and advise the AI on compatibility (e.g., "Source uses `anyhow`, Target uses `thiserror`. Snippets will need adaptation.").

### Phase 5: Synthesis & Industry Standards

*Goal: Extend the MCP server with pattern-aware tools and generate standardized AI entry points for every project.*

*Depends on: Phase 3.5 (base MCP server), Phase 4 (pattern data to serve and synthesize).*

*Success Metrics: MCP server extended with `get_project_dna` and `find_pattern` tools. `llms.txt` and `AGENTS.md` generated for all repos. `SYNTHESIS.md` produced on demand.*

1. **MCP Server Extensions (Pattern-Aware Tools):**

    * Build on the Phase 3.5 MCP server by adding pattern-intelligence tools:
        * `get_project_dna` — returns the Structural DNA map for a project.
        * `find_pattern(name="auth")` — searches Capability & Solution Index across all repos.
        * `compare_projects(a, b)` — returns a compatibility/migration pre-flight report.
        * `generate_situation_report(task)` — produces a full Situation Report for a given task.

2. **Standardized AI Entry Points:**

    * **`AGENTS.md`:** Generated per-project (covered in Phase 3). Phase 5 ensures these stay in sync with DNA and pattern data.
    * **`llms.txt`:** Adapted from the web standard (llmstxt.org) for local ecosystem use. The original `llms.txt` spec is a curated markdown sitemap designed for websites (complementing `robots.txt` and `sitemap.xml`). Toad adapts this concept by generating a `shadows/llms.txt` that acts as the **ecosystem-level table of contents** — a curated index of all projects with links to their `CONTEXT.md`, `AGENTS.md`, and DNA summaries. This is the file an AI agent reads *first* before diving deeper.

3. **Pattern Synthesis Logic:**

    * Allow Toad to aggregate "Inspiration" snippets from multiple sources into a `shadows/SYNTHESIS.md` file for deep analysis by the user's AI assistant.
    * This is the multi-repo equivalent of tools like `codebase-digest` (which packs a single repo for LLM consumption). Toad's synthesis spans the entire ecosystem.

---

## Notes & Ideas

* **Idea:** A "Semantic Cache" that stores the LLM's previous observations about a project to avoid re-explaining architecture. This maps to the "write/persist" pattern in context engineering — agents need to remember information from task to task.

* **Idea:** Exporting context in a format optimized for RAG (Retrieval-Augmented Generation). Subsumed by Phase 5's synthesis and `llms.txt` generation.

* **Inference:** Improving `essence` extraction to include architectural patterns (e.g., "Modular Rust Workspace", "FastAPI Backend") will significantly boost AI's ability to plan changes.

* **Requirement:** Cross-repo analysis requires a lightweight index of "where things are" to avoid token-heavy full searches. This is the Atlas / Structural DNA.

* **Industry Context:** The shift from RAG to Context Engineering (Lance Martin / LangChain, 2025) emphasizes four patterns: **write** (persist memory), **compress** (summarize/prune), **isolate** (split across agents), and **select** (retrieve the right context). Toad's phases map directly: Phase 2 = write, tiered prompts = compress, MCP = select, DNA/Atlas = isolate.

* **Industry Context:** Windsurf's Varun Mohan describes their retrieval as "a combination of grep/file search, knowledge graph based retrieval, and a re-ranking step." Toad's Atlas + taxonomy + semantic reveal mirrors this architecture at the ecosystem level.

* **Industry Context:** The `AGENTS.md` format (agents.md) is supported by 20+ AI tools and adopted by 60k+ projects. Generating these from Toad's data is a high-leverage, low-effort win.

* **Industry Context:** Google's internal code migration research (arxiv:2501.06972, 2025) validates the **discover → generate context → transform → validate** workflow. The hardest part of large-scale migrations is not the code change itself but **finding where to make changes**. Toad's Capability & Solution Index (Phase 4) and Situation Reports directly address this "opportunity discovery" problem for solo developers.

* **Industry Context:** CodePrism (Rustic AI) demonstrates that cross-file architectural patterns can be identified with confidence scoring — detecting service layers, data access patterns, and error handling strategies. Toad achieves similar results at the ecosystem level without AST parsing, using heuristic-based directory/file naming conventions.

* **Industry Context:** `codebase-digest` (1.2k+ GitHub stars) proves demand for "pack a codebase for LLM consumption." Toad's `SYNTHESIS.md` is the multi-repo evolution of this concept.

* **Industry Context:** The `llms.txt` spec (llmstxt.org) standardizes how websites expose curated content to LLMs. Toad adapts this for local ecosystems — `shadows/llms.txt` serves as the ecosystem sitemap that agents read first.
